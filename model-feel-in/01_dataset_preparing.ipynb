{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbdcd038-c913-4b42-96b4-b8059b525c00",
   "metadata": {},
   "source": [
    "# Dataset Preparing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40196d94-b1ca-4d6f-914a-260055c55c05",
   "metadata": {},
   "source": [
    "# 01 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "737fc9b2-9be2-4ba5-93f7-26152421dd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import preprocessor as p\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c24a1f-f504-4388-be0e-cc926425c134",
   "metadata": {},
   "source": [
    "# 02 Dataset List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f52f2f-130d-49b9-a33f-2f26269bfa44",
   "metadata": {},
   "source": [
    "## OSCAR Indonesia 2019 Deduplicated\n",
    "\n",
    "OSCAR or Open Super-large Crawled Aggregated coRpus is a huge multilingual corpus obtained by language classification and filtering of the Common Crawl corpus using the Ungoliant architecture.\n",
    "\n",
    "\n",
    "https://huggingface.co/datasets/oscar<br>\n",
    "https://oscar-corpus.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feed9316-1dca-4264-8ee2-7da62c83bdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset oscar (/home/tel-user/.cache/huggingface/datasets/oscar/unshuffled_deduplicated_id/1.0.0/84838bd49d2295f62008383b05620571535451d84545037bb94d6f3501651df2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "226b8fd9bbc84813a8c22cccd1526ef5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "oscard_id_dataset = load_dataset('oscar', 'unshuffled_deduplicated_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc85d9e3-cde0-4ba0-a63a-e5d7976fa018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'text'],\n",
       "        num_rows: 9948521\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oscard_id_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2adbedc7-dcd6-4fed-a7c7-b55d06fa9ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': Value(dtype='int64', id=None), 'text': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oscard_id_dataset['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7f19992-181a-4026-a788-4d4c2ffae55a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1,\n",
       " 'text': 'Perihal dari itu, kalau kunci hal yang demikian hilang, pemilik wajib melapor ke bengkel sah untuk dibuatkan kunci baru dengan kode baru sekalian pendaftaran. Biayanya tidak murah, melainkan rupanya dapat lebih rendah daripada membikin ke spesialis duplikat kunci. Rata-rata biaya pembuatan satu kunci immobilizer sekitar Rp 1.000.000-an. Itu termasuk kunci, jasa, dan tarif registrasi, serta mencetak mata kuncinya.\\nCara pasang immobilizer mt25 di Madiun Catat saja no ponsel kami mungkin sewaktu waktu anda memerlukan jasa pakar kunci dengan biaya yang terjangkau dan dengan pelayanan memuaskan. Kami siap 24 jam online melewati medsos juga bisa dihubungi. Pokoknya soal keadaan sulit kunci apapun percayakan terhadap kami.\\nBagaimana membuka kunci mobil tanpa merusak ? – Melainkan kali saat kendaraan beroda empat kita kehilangan kuncinya kita mengundang jasa tukang kunci mobil dan mempercayakan kendaraan beroda empat kita terhadap mereka, Hakekatnya apa yang terjadi kepercayaan yang kita berikan malahan disalah pakai dan melakukan perkerjaan dengan kacau atau malahan membikin mobil kita semakin hancur. Spesialis ada metode untuk membuka pintu mobil tanpa merusaknya yaitu dengan Membuka Kunci Elektrik/Otomatis, Membuka kunci manual, masuk melalui bagasi.\\nPelbagai kunci imobilizer kota surabaya – Kunci Immobilizer ?, Ya kunci Immobilizer yakni salah satu cara kita menghindarkan mobil kita dari praktik pencurian cara ini juga dapat dikelompokkan kedalam cara yang lumayan canggih dan handal untuk mencegah pencurian kendaraan beroda empat, bagaimana bisa?.\\nBerikut penjelasannya jadi metode ini ketika anda memasukan kunci kendaraan beroda empat ke lubang kunci atau membawa kunci mobil mendekati mobil, maka kunci mentransmisikan Cara pasang immobilizer mt25 di Madiun kode elektronik ke kendaraan. Mesin mobil hanya akan bisa hidup bila kode transmisi pada kunci pantas dengan kode transmisi yang terdapat pada mobil. Sebab kode transmisi pada kunci tidak cocok dengan yang ada di mobil, maka pintu kendaraan beroda empat tidak bisa di buka. Tentunya mesin kendaraan beroda empat juga tidak bisa hidup walapun orang bisa masuk dalam kedalam kendaraan beroda empat dan memasukan kunci palsu ke lubang kunci.\\nKarena chip transponder yang memiliki kode sama, tertanam dalam kendaraan beroda empat dan kunci, bila terjadi kehilangan kunci kendaraan beroda empat, untuk menganti dengan yang baru akan memerlukan biaya lebih mahal dari kunci biasa. Penggantian kunci Immobilizer, sebaiknya dilaksanakan di bengkel resmi atau Karena specialis kunci Immobilizer kepercayaan anda. Lantas hubungi kami!'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample data\n",
    "oscard_id_dataset['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85543c94-e3a4-41a2-ad8f-8b88b76f504f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba3a679df9954e8da2cc551738c005f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9948521 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19min 48s, sys: 2min 48s, total: 22min 37s\n",
      "Wall time: 23min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "text_data = []\n",
    "file_count = 0\n",
    "\n",
    "for sample in tqdm(oscard_id_dataset['train']):\n",
    "    sample = sample['text'].lower().replace('\\n', '')\n",
    "    text_data.append(sample)\n",
    "    \n",
    "    if len(text_data) == 10_000:\n",
    "        with open(f'data/id_oscar/text_{file_count}.txt', 'w', encoding='utf-8') as fp:     # once 10K mark, save to file\n",
    "            fp.write('\\n'.join(text_data))\n",
    "        text_data = []\n",
    "        file_count += 1\n",
    "        \n",
    "with open(f'data/id_oscar/text_{file_count}.txt', 'w', encoding='utf-8') as fp:             # save leftover samples\n",
    "        fp.write('\\n'.join(text_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f0fde6a-8279-43da-9591-06a03bf0fecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/id_oscar/text_543.txt',\n",
       " 'data/id_oscar/text_155.txt',\n",
       " 'data/id_oscar/text_528.txt',\n",
       " 'data/id_oscar/text_582.txt',\n",
       " 'data/id_oscar/text_983.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oscard_id_dataset_path = [str(x) for x in Path('data/id_oscar').glob('**/*.txt')]\n",
    "\n",
    "# see file in path\n",
    "oscard_id_dataset_path[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bec5ee-e3b0-4f1b-b860-e9aea620e9ba",
   "metadata": {},
   "source": [
    "## IndoNLU: EmoT\n",
    "An emotion classification dataset collected from the social media platform Twitter [(Saputri et al., 2018)](https://ieeexplore.ieee.org/document/8629262)\n",
    "\n",
    "https://huggingface.co/datasets/indonlu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b019bab5-74b7-42f6-b04f-3b0aa031c9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset indonlu (/home/tel-user/.cache/huggingface/datasets/indonlu/emot/1.0.0/0a83b181cd831cd5d9c15ffe39f3be76af23407eba2c902bccca53fa905d68af)\n"
     ]
    }
   ],
   "source": [
    "emot_id_dataset = load_dataset('indonlu', 'emot', split='train+test+validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6faee058-1449-438c-9a2e-98733cd14f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tweet', 'label'],\n",
       "    num_rows: 4401\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emot_id_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a253dd45-e00d-4ce2-85c5-553abeefd5b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tweet': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(num_classes=5, names=['sadness', 'anger', 'love', 'fear', 'happy'], id=None)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emot_id_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04e747e2-361e-4096-9edb-8643ffd83da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tweet': '[USERNAME] [USERNAME] Dari pertama [USERNAME] menduduki bangku jabatan anda, rakyat belum pernah mendengar dan melihat hasil kerja dan prestasi nyata yang anda berikan semasa menduduki bangku jabatan.Coba tanya Kenapa [USERNAME] ? Abdi rakyat butuh seoran',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample data\n",
    "emot_id_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c992a1fd-73c4-45bd-82a7-8481b746a629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "507fa1aa43ce4aa4bba427139190ab3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4401 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 473 ms, sys: 12.2 ms, total: 485 ms\n",
      "Wall time: 479 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "text_data = []\n",
    "file_count = 0\n",
    "\n",
    "for sample in tqdm(emot_id_dataset):\n",
    "    sample = sample['tweet'].lower().replace('\\n', '')\n",
    "    text_data.append(sample)\n",
    "    \n",
    "    if len(text_data) == 10_000:\n",
    "        with open(f'data/id_emot/text_{file_count}.txt', 'w', encoding='utf-8') as fp:     # once 10K mark, save to file\n",
    "            fp.write('\\n'.join(text_data))\n",
    "        text_data = []\n",
    "        file_count += 1\n",
    "        \n",
    "with open(f'data/id_emot/text_{file_count}.txt', 'w', encoding='utf-8') as fp:             # save leftover samples\n",
    "        fp.write('\\n'.join(text_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad82e534-79e7-454b-be06-619a8ea1c575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/id_emot/text_0.txt']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emot_id_dataset_path = [str(x) for x in Path('data/id_emot').glob('**/*.txt')] \n",
    "\n",
    "# see file in path\n",
    "emot_id_dataset_path[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc1f5c0-2d1f-4799-8199-b8ef5a9d8e85",
   "metadata": {},
   "source": [
    "## IndoNLU: CASA\n",
    "An aspect-based sentiment analysis dataset consisting of around a thousand car reviews collected from multiple Indonesian online automobile platforms [(Ilmania et al., 2018)](https://ieeexplore.ieee.org/document/8629181)\n",
    "\n",
    "https://huggingface.co/datasets/indonlu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3a7b543-09b6-4fa0-9687-cd2353588e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset indonlu (/home/tel-user/.cache/huggingface/datasets/indonlu/casa/1.0.0/0a83b181cd831cd5d9c15ffe39f3be76af23407eba2c902bccca53fa905d68af)\n"
     ]
    }
   ],
   "source": [
    "casa_id_dataset = load_dataset('indonlu', 'casa', split='train+test+validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c8dcae9-e951-40a5-8778-b8ff28d3088f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'fuel', 'machine', 'others', 'part', 'price', 'service'],\n",
       "    num_rows: 1080\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "casa_id_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f69ce5b7-6b14-48d7-878a-88ae9d69dfc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': Value(dtype='string', id=None),\n",
       " 'fuel': ClassLabel(num_classes=3, names=['negative', 'neutral', 'positive'], id=None),\n",
       " 'machine': ClassLabel(num_classes=3, names=['negative', 'neutral', 'positive'], id=None),\n",
       " 'others': ClassLabel(num_classes=3, names=['negative', 'neutral', 'positive'], id=None),\n",
       " 'part': ClassLabel(num_classes=3, names=['negative', 'neutral', 'positive'], id=None),\n",
       " 'price': ClassLabel(num_classes=3, names=['negative', 'neutral', 'positive'], id=None),\n",
       " 'service': ClassLabel(num_classes=3, names=['negative', 'neutral', 'positive'], id=None)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "casa_id_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7e3d412-f286-40d1-be5f-0262c63567c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'Avanza kenapa jadi boros bensin begini dah ah. Baru diisi sudah mau setengah saja .',\n",
       " 'fuel': 0,\n",
       " 'machine': 1,\n",
       " 'others': 1,\n",
       " 'part': 1,\n",
       " 'price': 1,\n",
       " 'service': 1}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample data\n",
    "casa_id_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2980924e-5c46-4b3e-80e9-f80131d368a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2617345006c410388945c8cb6674031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1080 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 222 ms, sys: 4.96 ms, total: 227 ms\n",
      "Wall time: 239 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "text_data = []\n",
    "file_count = 0\n",
    "\n",
    "for sample in tqdm(casa_id_dataset):\n",
    "    sample = sample['sentence'].lower().replace('\\n', '')\n",
    "    text_data.append(sample)\n",
    "    \n",
    "    if len(text_data) == 10_000:\n",
    "        with open(f'data/id_casa/text_{file_count}.txt', 'w', encoding='utf-8') as fp:     # once 10K mark, save to file\n",
    "            fp.write('\\n'.join(text_data))\n",
    "        text_data = []\n",
    "        file_count += 1\n",
    "        \n",
    "with open(f'data/id_casa/text_{file_count}.txt', 'w', encoding='utf-8') as fp:             # save leftover samples\n",
    "        fp.write('\\n'.join(text_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "734b4d0f-f7a2-4c0f-ab3e-2ff306fe974e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/id_casa/text_0.txt']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "casa_id_dataset_path = [str(x) for x in Path('data/id_casa').glob('**/*.txt')]\n",
    "\n",
    "# see file in path\n",
    "casa_id_dataset_path[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3c7857-0d10-451f-a605-2be3cb5b2440",
   "metadata": {},
   "source": [
    "## IndoNLU: SmSA\n",
    "This sentence-level sentiment analysis dataset is a collection of comments and reviews in Indonesian obtained from multiple online platforms [(Purwarianti and Crisdayanti, 2019)](https://ieeexplore.ieee.org/document/8904199)\n",
    "\n",
    "https://huggingface.co/datasets/indonlu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7cc00114-ef57-4a62-9336-1964cfef0b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset indonlu (/home/tel-user/.cache/huggingface/datasets/indonlu/smsa/1.0.0/0a83b181cd831cd5d9c15ffe39f3be76af23407eba2c902bccca53fa905d68af)\n"
     ]
    }
   ],
   "source": [
    "smsa_id_dataset = load_dataset('indonlu', 'smsa', split='train+test+validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b2fa94c-21d2-4ba1-b944-48524d935934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 12760\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smsa_id_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e1f3a89-4077-4267-a595-60b30bfc1df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'simcard indosat inaktiv gara-gara lupa isi pulsa dan kabar nya aktif jika pinda ke pasca bayar , ribet banget',\n",
       " 'label': 2}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample data\n",
    "smsa_id_dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86d63c37-47cd-4267-aacb-2af5f4c125b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aff0186aaa741f5b82215922d38de45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.31 s, sys: 31.5 ms, total: 1.35 s\n",
      "Wall time: 1.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "text_data = []\n",
    "file_count = 0\n",
    "\n",
    "for sample in tqdm(smsa_id_dataset):\n",
    "    sample = sample['text'].lower().replace('\\n', '')\n",
    "    text_data.append(sample)\n",
    "    \n",
    "    if len(text_data) == 10_000:\n",
    "        with open(f'data/id_smsa/text_{file_count}.txt', 'w', encoding='utf-8') as fp:     # once 10K mark, save to file\n",
    "            fp.write('\\n'.join(text_data))\n",
    "        text_data = []\n",
    "        file_count += 1\n",
    "        \n",
    "with open(f'data/id_smsa/text_{file_count}.txt', 'w', encoding='utf-8') as fp:             # save leftover samples\n",
    "        fp.write('\\n'.join(text_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e91947d0-f8c5-440f-9ec6-c2592e68ee12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/id_smsa/text_1.txt', 'data/id_smsa/text_0.txt']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smsa_id_dataset_path = [str(x) for x in Path('data/id_smsa').glob('**/*.txt')]\n",
    "\n",
    "# see file in path\n",
    "smsa_id_dataset_path[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d442c2-8385-4365-b968-b20a98d799d3",
   "metadata": {},
   "source": [
    "## IndoNLU: HoASA\n",
    "An aspect-based sentiment analysis dataset consisting of hotel reviews collected from the hotel aggregator platform, AiryRooms [(Azhar et al., 2019)](https://ieeexplore.ieee.org/document/8988898)\n",
    "\n",
    "https://huggingface.co/datasets/indonlu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81850db5-4054-4a5e-bc48-476b75e8756a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset indonlu (/home/tel-user/.cache/huggingface/datasets/indonlu/hoasa/1.0.0/0a83b181cd831cd5d9c15ffe39f3be76af23407eba2c902bccca53fa905d68af)\n"
     ]
    }
   ],
   "source": [
    "hoasa_id_dataset = load_dataset('indonlu', 'hoasa', split='train+test+validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9e7a3442-86aa-4171-84a0-243700bfe89e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'ac', 'air_panas', 'bau', 'general', 'kebersihan', 'linen', 'service', 'sunrise_meal', 'tv', 'wifi'],\n",
       "    num_rows: 2854\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hoasa_id_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc5c23b9-040b-416f-b772-f042e38e8365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': Value(dtype='string', id=None),\n",
       " 'ac': ClassLabel(num_classes=4, names=['neg', 'neut', 'pos', 'neg_pos'], id=None),\n",
       " 'air_panas': ClassLabel(num_classes=4, names=['neg', 'neut', 'pos', 'neg_pos'], id=None),\n",
       " 'bau': ClassLabel(num_classes=4, names=['neg', 'neut', 'pos', 'neg_pos'], id=None),\n",
       " 'general': ClassLabel(num_classes=4, names=['neg', 'neut', 'pos', 'neg_pos'], id=None),\n",
       " 'kebersihan': ClassLabel(num_classes=4, names=['neg', 'neut', 'pos', 'neg_pos'], id=None),\n",
       " 'linen': ClassLabel(num_classes=4, names=['neg', 'neut', 'pos', 'neg_pos'], id=None),\n",
       " 'service': ClassLabel(num_classes=4, names=['neg', 'neut', 'pos', 'neg_pos'], id=None),\n",
       " 'sunrise_meal': ClassLabel(num_classes=4, names=['neg', 'neut', 'pos', 'neg_pos'], id=None),\n",
       " 'tv': ClassLabel(num_classes=4, names=['neg', 'neut', 'pos', 'neg_pos'], id=None),\n",
       " 'wifi': ClassLabel(num_classes=4, names=['neg', 'neut', 'pos', 'neg_pos'], id=None)}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hoasa_id_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01cc15e9-aced-4b2a-87d8-6a5c7b4c5af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'sangat mengecewakan... hotel bad image, kebersihan kurang, berisik',\n",
       " 'ac': 1,\n",
       " 'air_panas': 1,\n",
       " 'bau': 1,\n",
       " 'general': 1,\n",
       " 'kebersihan': 0,\n",
       " 'linen': 1,\n",
       " 'service': 1,\n",
       " 'sunrise_meal': 1,\n",
       " 'tv': 1,\n",
       " 'wifi': 1}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample data\n",
    "hoasa_id_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a5830ba-e570-4e95-997f-fbe67b04563e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ea374f3feb048efa87e26dc0127b82b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1080 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 216 ms, sys: 16.3 ms, total: 233 ms\n",
      "Wall time: 236 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "text_data = []\n",
    "file_count = 0\n",
    "\n",
    "for sample in tqdm(casa_id_dataset):\n",
    "    sample = sample['sentence'].lower().replace('\\n', '')\n",
    "    text_data.append(sample)\n",
    "    \n",
    "    if len(text_data) == 10_000:\n",
    "        with open(f'data/id_hoasa/text_{file_count}.txt', 'w', encoding='utf-8') as fp:     # once 10K mark, save to file\n",
    "            fp.write('\\n'.join(text_data))\n",
    "        text_data = []\n",
    "        file_count += 1\n",
    "        \n",
    "with open(f'data/id_hoasa/text_{file_count}.txt', 'w', encoding='utf-8') as fp:             # save leftover samples\n",
    "        fp.write('\\n'.join(text_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ca7faf3-a555-43d2-aaed-eada1cddb37f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/id_hoasa/text_0.txt']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hoasa_id_dataset_path = [str(x) for x in Path('data/id_hoasa').glob('**/*.txt')]\n",
    "\n",
    "# see file in path\n",
    "hoasa_id_dataset_path[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbcb835-3d12-4328-b3c8-659b0d383271",
   "metadata": {},
   "source": [
    "## Indonesia Wikipedia Dump\n",
    "\n",
    "https://dumps.wikimedia.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "322edc6e-adbb-43ae-b4e0-92951c410fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-3107b47b1c96581c\n",
      "Reusing dataset text (/home/tel-user/.cache/huggingface/datasets/text/default-3107b47b1c96581c/0.0.0/acc32f2f2ef863c93c2f30c52f7df6cc9053a1c2230b8d7da0d210404683ca08)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ef3c0a530d549f0aa0ce5c0c31dc334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wiki_id_dataset = load_dataset('text', data_files='data/raw/id-wiki-dump/id-wiki-dump-lower.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ece12c7-06af-45db-bd36-05f2d7655b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 463225\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_id_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ed8405ca-6baa-462b-8db5-30787c0c101f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'muhammad anwar el sadat adalah seorang politikus mesir yang menjabat sebagai presiden mesir ketiga dari oktober hingga pembunuhannya oleh perwira tentara fundamentalis pada oktober anwar adalah seorang senior anggota perwira bebas yang menggulingkan raja farouk dalam revolusi mesir dan orang kepercayaan dekat presiden gamal abdel nasser di mana dia menjabat sebagai wakil presiden dua kali dan dia menggantikannya sebagai presiden pada tahun pada tahun sadat dan menachem begin perdana menteri israel menandatangani perjanjian damai bekerja sama dengan presiden amerika serikat jimmy carter di mana mereka diakui dengan hadiah nobel perdamaian dalam sebelas tahun sebagai presiden ia mengubah lintasan mesir berangkat dari banyak prinsip politik dan ekonomi nasserisme melembagakan kembali sistem multi partai dan meluncurkan kebijakan ekonomi infitah sebagai presiden ia memimpin mesir dalam perang yom kippur tahun untuk merebut kembali semenanjung sinai mesir yang telah diduduki israel sejak perang enam hari tahun menjadikannya pahlawan di mesir dan untuk sementara waktu dunia arab yang lebih luas setelah itu ia terlibat dalam negosiasi dengan israel yang berpuncak pada perjanjian perdamaian mesir israel ini memenangkan dia dan menachem begin hadiah nobel perdamaian membuat sadat pemenang nobel muslim pertama meskipun reaksi terhadap perjanjian yang mengakibatkan kembalinya sinai ke mesir umumnya menguntungkan di kalangan orang mesir perjanjian itu ditolak oleh ikhwanul muslimin dan kaum kiri yang merasa sadat telah mengabaikan upaya untuk memastikan negara palestina dengan pengecualian sudan dunia arab dan organisasi pembebasan palestina plo sangat menentang upaya sadat untuk membuat perdamaian terpisah dengan israel tanpa konsultasi terlebih dahulu dengan negara negara arab penolakannya untuk berdamai dengan mereka atas masalah palestina mengakibatkan mesir diskors dari liga arab perjanjian damai juga merupakan salah satu faktor utama yang menyebabkan pembunuhannya pada oktober militan yang dipimpin oleh khalid islambouli menembaki sadat dengan senapan otomatis selama parade oktober di kairo membunuhnya latar belakang sadat dilahirkan di mit abu al kum al minufiyah mesir dalam sebuah keluarga mesir sudan yang miskin dengan saudara laki laki dan perempuan ayahnya adalah seorang mesir sementara ibunya orang sudan salah seorang saudara laki lakinya atef sadat kelak menjadi seorang pilot dan terbunuh dalam perang yom kippur pada oktober anwar lulus dari akademi militer kerajaan di kairo pada dan ditempatkan di korps isyarat ia bergabung dengan gerakan perwira bebas yang bertekad untuk membebaskan mesir dari kekuasaan britania raya pada perang dunia ii ia dipenjarakan oleh britania atas usaha usahanya untuk mendapatkan bantuan dari kekuatan poros dalam mengusir pasukan pasukan pendudukan britania ia ikut serta dalam kudeta yang menggulingkan raja farouk ii ketika revolusi meletus ia diperintahkan mengambil alih jaringan radio dan mengumumkan pecahnya revolusi kepada rakyat mesir pada setelah memegang berbagai jabatan dalam pemerintahan mesir ia dipilih oleh presiden gamal abdel nasser untuk menjabat sebagai wakil presiden ia menduduki jabatan itu hingga dan sekali lagi dari hingga setelah nasser meninggal anwar sadat dilantik menjadi presiden pada tahun anwar sadat bersama sama dengan hafez al assad syria memimpin mesir dalam perang yom kippur melawan israel untuk merebut kembali semenanjung sinai yang dicaplok oleh israel ketika krisis terusan suez dan perang enam hari meskipun dalam pertempuran ini masih dipertentangkan pihak menang ataupun kalah serta hasil perjanjian camp david yang menetapkan sinai kembali ketangan mesir keberhasilan anwar sadat menaikan moral rakyat mesir dan dunia arab serta mengadakan untuk perjanjian damai beberapa tahun berikutnya perjanjian damai camp david yang diprakarsai jimmy carter dan henry kissinger memang mengembalikan wilayah mesir yang sebelumnya direbut oleh israel pada perang namun tidak mengembalikan dataran tinggi golan yang direbut israel kepada syria pada perang meski secara politik perang yom kippur atau perang ramadhan itu menguntungkan dunia arab masalah palestina dan jerusalem terutama jerusalem timur yang direbut israel pada perang masih mengganjal bahkan beberapa kalangan mengatakan dilupakan hal ini membuat kemarahan dari kalangan plo kaum fundamentalis dan pergerakan islam dan kalangan palestina serta dunia arab terutama setelah kunjungannya ke jerussalem atas undangan manachem begin pada tahun anwar sadat mengadakan kunjungan ke jerusalem atas undangan perdana menteri israel menachem begin yang merupakan awal dari perundingan perdamaian antara israel dan mesir pada tahun terciptalah perjanjian damai camp david yang mana anwar sadat dan menachem begin menerima hadiah nobel perdamaian bagaimanapun tindakan ini ditentang hebat oleh dunia arab banyak yang percaya bahwa hanya dengan ancaman militer dapat memaksa israel berunding mengenai palestina dan perjanjian damai camp david menepikan mesir yang dianggap kekuatan militer di dunia arab yang signifikan disamping syria dan irak pada saat itu pada september anwar sadat mengenakan tindakan represif kepada organisasi pergerakan islam yang diaggapnya fundamentalis termasuk kumpulan pelajar dan organisasi koptik yang dianggapnya dapat mengganggu stabilitas nasional mesir dengan mengadakan tindakan penangkapan dan penahanan menyebabkan dia dikecam diseluruh dunia di atas pelanggaran ham dalam tindakannya itu pada oktober presiden anwar sadat tewas ditembak dalam sebuah parade militer oleh anggota tentara anggota jihad islam ini merupakan organisasi muslim mesir berhaluan keras yang menentang perjanjian damai mesir dengan israel tindakan represif anggota jihad islam terlihat dalam peristiwa september anwar sadat kemudian digantikan oleh wakil presiden hosni mubarak catatan kaki referensi kategori presiden mesir kategori perdana menteri mesir kategori pemenang hadiah nobel perdamaian kategori kepala negara yang dibunuh kategori person of the year kategori pemimpin perang dingin kategori tokoh militer mesir kategori kolonel'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample data\n",
    "wiki_id_dataset['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "94ac914c-7446-4702-8b75-2454b61ea5d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "362404120b284fa2be92fbad08f33166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/463225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 54.8 s, sys: 8.7 s, total: 1min 3s\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "text_data = []\n",
    "file_count = 0\n",
    "\n",
    "for sample in tqdm(wiki_id_dataset['train']):\n",
    "    sample = sample['text'].lower().replace('\\n', '')\n",
    "    text_data.append(sample)\n",
    "    \n",
    "    if len(text_data) == 10_000:\n",
    "        with open(f'data/id_wiki/text_{file_count}.txt', 'w', encoding='utf-8') as fp:     # once 10K mark, save to file\n",
    "            fp.write('\\n'.join(text_data))\n",
    "        text_data = []\n",
    "        file_count += 1\n",
    "        \n",
    "with open(f'data/id_wiki/text_{file_count}.txt', 'w', encoding='utf-8') as fp:             # save leftover samples\n",
    "        fp.write('\\n'.join(text_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "79e86fd3-0c5c-495e-863e-6930da2ea6c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/id_wiki/text_34.txt',\n",
       " 'data/id_wiki/text_22.txt',\n",
       " 'data/id_wiki/text_25.txt',\n",
       " 'data/id_wiki/text_26.txt',\n",
       " 'data/id_wiki/text_16.txt']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_id_dataset_path = [str(x) for x in Path('data/id_wiki').glob('**/*.txt')]\n",
    "\n",
    "# see file in path\n",
    "wiki_id_dataset_path[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a869303a-e549-43f3-84f3-fda13729e7c9",
   "metadata": {},
   "source": [
    "## Indonesian Hate Speech & Abusive Language Dataset\n",
    "\n",
    "https://github.com/okkyibrohim/id-multi-label-hate-speech-and-abusive-language-detection (Ibrohim and Budi, 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c48aa71f-5028-4688-bc11-bdd60c860292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/okkyibrohim/id-multi-label-hate-speech-and-abusive-language-detection/master/re_dataset.csv -O data/raw/abusive-id.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a0967bc-3edb-41d3-bd3f-92eda481e19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "abusive_id_df = pd.read_csv('data/raw/abusive-id.csv', on_bad_lines='skip', encoding='latin1')\n",
    "abusive_id_dataset = Dataset.from_pandas(abusive_id_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "37fb3781-a846-45c0-b04c-7a20519904c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Tweet', 'HS', 'Abusive', 'HS_Individual', 'HS_Group', 'HS_Religion', 'HS_Race', 'HS_Physical', 'HS_Gender', 'HS_Other', 'HS_Weak', 'HS_Moderate', 'HS_Strong'],\n",
       "    num_rows: 13169\n",
       "})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abusive_id_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "031a372b-89d5-4228-bfb1-a3db7e9b2ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Tweet': Value(dtype='string', id=None),\n",
       " 'HS': Value(dtype='int64', id=None),\n",
       " 'Abusive': Value(dtype='int64', id=None),\n",
       " 'HS_Individual': Value(dtype='int64', id=None),\n",
       " 'HS_Group': Value(dtype='int64', id=None),\n",
       " 'HS_Religion': Value(dtype='int64', id=None),\n",
       " 'HS_Race': Value(dtype='int64', id=None),\n",
       " 'HS_Physical': Value(dtype='int64', id=None),\n",
       " 'HS_Gender': Value(dtype='int64', id=None),\n",
       " 'HS_Other': Value(dtype='int64', id=None),\n",
       " 'HS_Weak': Value(dtype='int64', id=None),\n",
       " 'HS_Moderate': Value(dtype='int64', id=None),\n",
       " 'HS_Strong': Value(dtype='int64', id=None)}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abusive_id_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "caed673a-baaa-446e-a339-259bec096385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Tweet': \"RT USER: USER siapa yang telat ngasih tau elu?edan sarap gue bergaul dengan cigax jifla calis sama siapa noh licew juga'\",\n",
       " 'HS': 0,\n",
       " 'Abusive': 1,\n",
       " 'HS_Individual': 0,\n",
       " 'HS_Group': 0,\n",
       " 'HS_Religion': 0,\n",
       " 'HS_Race': 0,\n",
       " 'HS_Physical': 0,\n",
       " 'HS_Gender': 0,\n",
       " 'HS_Other': 0,\n",
       " 'HS_Weak': 0,\n",
       " 'HS_Moderate': 0,\n",
       " 'HS_Strong': 0}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample data\n",
    "abusive_id_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b0fa3fae-54fd-4d6c-a3a6-b2bb44f7e19a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0836f26031904cf2bb61bd561ae3a155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13169 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.13 s, sys: 30.4 ms, total: 3.16 s\n",
      "Wall time: 3.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "text_data = []\n",
    "file_count = 0\n",
    "\n",
    "for sample in tqdm(abusive_id_dataset):\n",
    "    sample = sample['Tweet'].lower().replace('\\n', '')\n",
    "    text_data.append(sample)\n",
    "    \n",
    "    if len(text_data) == 10_000:\n",
    "        with open(f'data/id_abusive/text_{file_count}.txt', 'w', encoding='utf-8') as fp:     # once 10K mark, save to file\n",
    "            fp.write('\\n'.join(text_data))\n",
    "        text_data = []\n",
    "        file_count += 1\n",
    "        \n",
    "with open(f'data/id_abusive/text_{file_count}.txt', 'w', encoding='utf-8') as fp:             # save leftover samples\n",
    "        fp.write('\\n'.join(text_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3e1c0a9c-762f-47be-b275-858cfbdaac0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/id_abusive/text_1.txt', 'data/id_abusive/text_0.txt']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abusive_id_dataset_path = [str(x) for x in Path('data/id_abusive').glob('**/*.txt')]\n",
    "\n",
    "# see file in path\n",
    "abusive_id_dataset_path[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bc725f-dcc8-4ab9-8a55-3908bde9e46c",
   "metadata": {},
   "source": [
    "## Indonesian Hate Speech Language Dataset\n",
    "\n",
    "https://github.com/ialfina/id-hatespeech-detection (Alfina et al., 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3386cb15-0bf6-4d6d-8be2-2fc0a5e29037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/ialfina/id-hatespeech-detection/master/IDHSD_RIO_unbalanced_713_2017.txt -O data/raw/hatespeech-id.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "78f2fad6-11c5-4f6b-b340-3ae1334a5340",
   "metadata": {},
   "outputs": [],
   "source": [
    "hatespeech_id_df = pd.read_csv('data/raw/hatespeech-id.csv', sep='\\t', on_bad_lines='skip', encoding='latin1')\n",
    "hatespeech_id_dataset = Dataset.from_pandas(hatespeech_id_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8938f6f0-f101-4c9b-8987-3088ebd338bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Label', 'Tweet'],\n",
       "    num_rows: 713\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hatespeech_id_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2e0f2e18-fb0f-4ca8-a4fa-758f6dba653c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Label': Value(dtype='string', id=None),\n",
       " 'Tweet': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hatespeech_id_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0c8512bd-60e9-4c52-868b-c32645acd934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Label': 'Non_HS',\n",
       " 'Tweet': 'RT @baguscondromowo: Mereka terus melukai aksi dalam rangka memenjarakan Ahok atau Ahok gagal dalam Pilkada.'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample data\n",
    "hatespeech_id_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e30b2214-9769-4db1-91e8-19ed22cc1a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @baguscondromowo: Mereka terus melukai aksi dalam rangka memenjarakan Ahok atau Ahok gagal dalam Pilkada.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hatespeech_id_dataset['Tweet'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fa6236f4-6ab6-47cd-b1b8-6dc167013596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5305b39930914a7397d1b5a374aad786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/713 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 221 ms, sys: 3.25 ms, total: 224 ms\n",
      "Wall time: 272 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "text_data = []\n",
    "file_count = 0\n",
    "\n",
    "for sample in tqdm(hatespeech_id_dataset):\n",
    "    sample = sample['Tweet'].lower().replace('\\n', '')\n",
    "    sample = re.sub(r'http\\S+', '', sample) \n",
    "    sample = re.sub(r'[-+]?[0-9]+', '', sample)       \n",
    "    p.set_options(p.OPT.MENTION, p.OPT.RESERVED, p.OPT.HASHTAG, p.OPT.URL)\n",
    "    sample = p.clean(sample)\n",
    "    \n",
    "    text_data.append(sample)\n",
    "    \n",
    "    if len(text_data) == 10_000:\n",
    "        with open(f'data/id_hatespeech/text_{file_count}.txt', 'w', encoding='utf-8') as fp:     # once 10K mark, save to file\n",
    "            fp.write('\\n'.join(text_data))\n",
    "        text_data = []\n",
    "        file_count += 1\n",
    "        \n",
    "with open(f'data/id_hatespeech/text_{file_count}.txt', 'w', encoding='utf-8') as fp:             # save leftover samples\n",
    "        fp.write('\\n'.join(text_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "914fa496-a0b4-404b-a3f4-33a4ac8e11db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/id_hatespeech/text_0.txt']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hatespeech_id_dataset_path = [str(x) for x in Path('data/id_hatespeech').glob('**/*.txt')]\n",
    "\n",
    "# see file in path\n",
    "hatespeech_id_dataset_path[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84610d15-59ea-44ad-9090-03b86a5eb998",
   "metadata": {},
   "source": [
    "## Scraping Twitter\n",
    "Jan 1, 2020 - Dec 31, 2022 (4.500.000 indonesian tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "790c4e42-aac8-4af2-ba53-7d7bf5a8a7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-8224581112fba585\n",
      "Reusing dataset csv (/home/tel-user/.cache/huggingface/datasets/csv/default-8224581112fba585/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d144d6160d6b46bf93e2c883a1ca9116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tweet_id_dataset = load_dataset('csv', data_files='data/raw/id-tweet-dump/id-tweet-dump-clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b6af1989-77ad-4d00-a89a-3107327b5652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'tweet'],\n",
       "        num_rows: 3126987\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_id_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "513a72bf-138d-4211-997e-de7285a585cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': 1, 'tweet': 'Lihat, kehidupanku jauh lebih baik kan. Maaf ya.'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample data\n",
    "tweet_id_dataset['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f1e62a7f-d535-4fdb-b5d0-e4c7cfea192b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d7bd12185d4fbbb07f2af227920c15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3126987 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 43s, sys: 3.35 s, total: 4min 47s\n",
      "Wall time: 5min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "text_data = []\n",
    "file_count = 0\n",
    "\n",
    "for sample in tqdm(tweet_id_dataset['train']):\n",
    "    sample = sample['tweet'].lower().replace('\\n', '')\n",
    "    text_data.append(sample)\n",
    "    \n",
    "    if len(text_data) == 10_000:\n",
    "        with open(f'data/id_tweet/text_{file_count}.txt', 'w', encoding='utf-8') as fp:     # once 10K mark, save to file\n",
    "            fp.write('\\n'.join(text_data))\n",
    "        text_data = []\n",
    "        file_count += 1\n",
    "        \n",
    "with open(f'data/id_tweet/text_{file_count}.txt', 'w', encoding='utf-8') as fp:             # save leftover samples\n",
    "        fp.write('\\n'.join(text_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d5c599dd-cb2b-4f09-b2e7-18651ee6856f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/id_tweet/text_155.txt',\n",
       " 'data/id_tweet/text_246.txt',\n",
       " 'data/id_tweet/text_222.txt',\n",
       " 'data/id_tweet/text_225.txt',\n",
       " 'data/id_tweet/text_34.txt']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_id_dataset_dataset_path = [str(x) for x in Path('data/id_tweet').glob('**/*.txt')]\n",
    "\n",
    "# see file in path\n",
    "tweet_id_dataset_dataset_path[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb82a93b-6571-4df6-a0a9-776d73d9bc8e",
   "metadata": {},
   "source": [
    "# Merge All Dataset Folder Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7979a5a1-46bb-4c63-a26b-59ae89026531",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/id_oscar/text_543.txt',\n",
       " 'data/id_oscar/text_155.txt',\n",
       " 'data/id_oscar/text_528.txt',\n",
       " 'data/id_oscar/text_582.txt',\n",
       " 'data/id_oscar/text_983.txt',\n",
       " 'data/id_oscar/text_919.txt',\n",
       " 'data/id_oscar/text_729.txt',\n",
       " 'data/id_oscar/text_857.txt',\n",
       " 'data/id_oscar/text_246.txt',\n",
       " 'data/id_oscar/text_222.txt']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dataset_path = oscard_id_dataset_path + emot_id_dataset_path + casa_id_dataset_path + smsa_id_dataset_path + hoasa_id_dataset_path + wiki_id_dataset_path + abusive_id_dataset_path + hatespeech_id_dataset_path + tweet_id_dataset_dataset_path\n",
    "\n",
    "# see merge path\n",
    "all_dataset_path[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cea6c7f3-68b0-4367-8688-dd42acd1f2c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1363"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "004b63c8-f57f-4885-82c4-db559f5e4879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# dump folder path\n",
    "with open('data/all_dataset_path', 'wb') as fp:\n",
    "    pickle.dump(all_dataset_path, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:indo-emo-bert] *",
   "language": "python",
   "name": "conda-env-indo-emo-bert-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
